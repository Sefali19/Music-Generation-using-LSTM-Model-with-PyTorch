# -*- coding: utf-8 -*-
"""Music_gen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUvndoG9TGWTceNj4qBKqJxAWR8YLZHZ
"""

!pip install pretty_midi
!sudo apt install -y fluidsynth
!pip install --upgrade pyfluidsynth

import numpy as np
import tensorflow as tf
import pandas as pd
import collections
import fluidsynth
import glob
import pretty_midi
from IPython import display
from typing import Dict, List, Optional, Sequence, Tuple

import zipfile
import os

# Define paths
zip_file_path = '/content/drive/MyDrive/Mozart.zip'  # Change this to the path of your zip file
extract_to_path = '/content/music-midi-dataset'  # Directory where you want to extract the contents

# Create target directory if it doesn't exist
os.makedirs(extract_to_path, exist_ok=True)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to_path)

print(f"Data extracted to {extract_to_path}")

import IPython.display as display

# Define the function to display audio
def display_audio(pm, seconds=30, sampling_rate=44100):
    waveform = pm.fluidsynth(fs=sampling_rate)
    # Take a sample of the generated waveform to mitigate kernel resets
    waveform_short = waveform[:seconds * sampling_rate]
    return display.Audio(waveform_short, rate=sampling_rate)

# Load a MIDI file
midi_file_path = '/content/music-midi-dataset/mz_311_1.mid'  # Change this to the path of your MIDI file
pm = pretty_midi.PrettyMIDI(midi_file_path)

# Print instruments in the MIDI file
print(pm.instruments)
instrument = pm.instruments[0]

# Display audio
audio = display_audio(pm)
display.display(audio)

def midi_to_notes(midi_file):
	pm = pretty_midi.PrettyMIDI(midi_file)
	instrument = pm.instruments[0]
	notes = collections.defaultdict(list)
	sorted_notes = sorted(instrument.notes , key=lambda note:note.start)
	prev_start = sorted_notes[0].start

	for note in sorted_notes:
		start = note.start
		end = note.end
		notes["pitch"].append(note.pitch)
		notes["start"].append(start)
		notes["end"].append(end)
		notes["step"].append(start - prev_start)
		notes["duration"].append(end - start)
		prev_start = start
	return pd.DataFrame({name:np.array(value) for name,value in notes.items()})

def notes_to_midi(
    notes: pd.DataFrame,
    out_file: str,
    instrument_name: str,
    velocity: int = 100, # note loudness
) -> pretty_midi.PrettyMIDI:

    pm = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(
        program=pretty_midi.instrument_name_to_program(instrument_name)
    )

    prev_start = 0
    for i, note in notes.iterrows():
        start = float(prev_start + note['step'])
        end = float(start + note['duration'])
        midi_note = pretty_midi.Note(
            velocity=velocity,
            pitch=int(note['pitch']),
            start=start,
            end=end,
        )
        instrument.notes.append(midi_note)
        prev_start = start

    pm.instruments.append(instrument)
    pm.write(out_file)
    return pm

# Example usage:
# Create a DataFrame with some sample note data
sample_notes = pd.DataFrame({
    'pitch': [60, 62, 64, 65, 67],  # MIDI note numbers
    'step': [0.0, 0.5, 0.5, 0.5, 0.5],  # time between notes
    'duration': [0.5, 0.5, 0.5, 0.5, 0.5]  # duration of each note
})

# Convert notes to MIDI
out_midi_file = 'output.mid'
pm = notes_to_midi(sample_notes, out_midi_file, 'Acoustic Grand Piano')

# Verify by loading and playing the MIDI file (optional)
# pm = pretty_midi.PrettyMIDI(out_midi_file)
# display.display(display.Audio(pm.fluidsynth(), rate=44100))

# Load notes from multiple MIDI files
num_files = 21
filenames = glob.glob('/content/music-midi-dataset/*.mid')
all_notes = []
for f in filenames[:num_files]:
    notes = midi_to_notes(f)
    all_notes.append(notes)
all_notes = pd.concat(all_notes)

print(all_notes)

# Prepare the dataset for TensorFlow
key_order = ["pitch", "step", "duration"]
train_notes = np.stack([all_notes[key] for key in key_order], axis=1)
notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)

# Print the element specification of the dataset
print(notes_ds.element_spec)

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd

# Define the custom Dataset class
class NotesDataset(Dataset):
    def __init__(self, sequences, targets):
        self.sequences = sequences
        self.targets = targets

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        target = self.targets[idx]
        return sequence, {
            "pitch": target[0],
            "step": target[1],
            "duration": target[2]
        }

# Define the function to create sequences
def create_sequences(train_notes, seq_length, vocab_size=128):
    sequences = []
    targets = []
    num_seq = train_notes.shape[0] - seq_length
    for i in range(num_seq):
        sequence = train_notes[i:i+seq_length - 1, :] / [vocab_size, 1, 1]
        target = train_notes[i + seq_length] / vocab_size
        sequences.append(sequence)
        targets.append(target)
    sequences = np.array(sequences)
    targets = np.array(targets)
    print(sequences.shape, targets.shape)
    return sequences, targets

# Example notes data (replace with your actual notes data)
train_notes = np.random.randint(0, 128, size=(1000, 3))  # Replace with actual notes data

# Create sequences and targets
seq_length = 20
vocab_size = 128
sequences, targets = create_sequences(train_notes, seq_length, vocab_size)

# Create the PyTorch dataset
notes_dataset = NotesDataset(sequences, targets)

# Define the DataLoader
batch_size = 64
buffer_size = 5000
train_loader = DataLoader(notes_dataset, batch_size=batch_size, shuffle=True)

# Print the first batch to verify
for batch in train_loader:
    sequences, targets = batch
    print(sequences.shape)
    print(targets)
    break

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np

# Define the custom PyTorch model
# class MusicLSTMModel(nn.Module):
#     def __init__(self, seq_length, input_dim=3, lstm_units=128, pitch_classes=64):
#         super(MusicLSTMModel, self).__init__()
#         self.lstm = nn.LSTM(input_dim, lstm_units, batch_first=True)
#         self.pitch_dense = nn.Linear(lstm_units, pitch_classes)
#         self.step_dense = nn.Linear(lstm_units, 1)
#         self.duration_dense = nn.Linear(lstm_units, 1)

#     def forward(self, x):
#         _, (h_n, _) = self.lstm(x)
#         h_n = h_n[-1]
#         pitch = self.pitch_dense(h_n)
#         step = self.step_dense(h_n)
#         duration = self.duration_dense(h_n)
#         return {"pitch": pitch, "step": step, "duration": duration}

class MusicLSTMModel(nn.Module):
    def __init__(self, seq_length, input_dim=3, lstm_units=128, pitch_classes=128):
        super(MusicLSTMModel, self).__init__()
        self.dropout = nn.Dropout(0.2)
        self.lstm1 = nn.LSTM(input_dim, lstm_units, batch_first=True)
        self.lstm2 = nn.LSTM(lstm_units, lstm_units, batch_first=True)
        self.lstm3 = nn.LSTM(lstm_units, lstm_units, batch_first=True)
        self.pitch_dense = nn.Linear(lstm_units, pitch_classes)
        self.step_dense = nn.Linear(lstm_units, 1)
        self.duration_dense = nn.Linear(lstm_units, 1)

    def forward(self, x):
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, (h_n, _) = self.lstm3(x)
        h_n = h_n[-1]
        pitch = self.pitch_dense(h_n)
        step = self.step_dense(h_n)
        duration = self.duration_dense(h_n)
        return {"pitch": pitch, "step": step, "duration": duration}

# Set parameters
seq_length = 20
vocab_size = 128
learning_rate = 0.005
batch_size = 64

# Initialize the model
model = MusicLSTMModel(seq_length)

# Define the loss functions
pitch_loss_fn = nn.CrossEntropyLoss()
step_loss_fn = nn.MSELoss()
duration_loss_fn = nn.MSELoss()

# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
model

# Example data (replace with actual dataset)
# train_notes = np.random.randint(0, 128, size=(1000, 3))  # Replace with actual notes data
# sequences, targets = create_sequences(train_notes, seq_length, vocab_size)
# notes_dataset = NotesDataset(sequences, targets)
# train_loader = DataLoader(notes_dataset, batch_size=batch_size, shuffle=True)

# Training loop
num_epochs = 100  # Set the number of epochs
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        sequences, targets = batch
        sequences = sequences.float()  # Ensure the input is float
        targets_pitch = targets["pitch"].long()  # Ensure the targets are long for CrossEntropyLoss
        targets_step = targets["step"].float()  # Ensure the targets are float for MSELoss
        targets_duration = targets["duration"].float()  # Ensure the targets are float for MSELoss

        # Forward pass
        outputs = model(sequences)
        pitch_output = outputs["pitch"]
        step_output = outputs["step"]
        duration_output = outputs["duration"]

        # Compute the losses
        pitch_loss = pitch_loss_fn(pitch_output, targets_pitch)
        step_loss = step_loss_fn(step_output.squeeze(), targets_step)
        duration_loss = duration_loss_fn(duration_output.squeeze(), targets_duration)

        # Total loss with weights
        total_loss = 0.05 * pitch_loss + step_loss + duration_loss

        # Backward pass and optimization
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        total_loss += total_loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')

print("Training complete")

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd

# Define the predict_next_note function
def predict_next_note(notes, pytorch_model, temperature):
    assert temperature > 0
    inputs = torch.tensor(notes, dtype=torch.float32).unsqueeze(0)
    with torch.no_grad():
        predictions = pytorch_model(inputs)

    pitch_logits = predictions['pitch']
    step = predictions['step']
    duration = predictions['duration']

    pitch_logits = pitch_logits / temperature
    pitch_probs = F.softmax(pitch_logits, dim=-1)
    pitch = torch.multinomial(pitch_probs, num_samples=1)

    pitch = pitch.squeeze().item()
    duration = duration.squeeze().item()
    step = step.squeeze().item()

    step = max(0, step)
    duration = max(0, duration)

    return int(pitch), float(step), float(duration)

# Example usage
temperature = 2.0
num_predictions = 1200

# Assuming raw_notes and key_order are defined as before
sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# The initial sequence of notes and the pitch is normalized similar to training sequences
input_notes = (sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))

generated_notes = []
prev_start = 0

for _ in range(num_predictions):
    pitch, step, duration = predict_next_note(input_notes, model, temperature)
    start = prev_start + step
    end = start + duration
    input_note = (pitch, step, duration)
    generated_notes.append((*input_note, start, end))

    input_notes = np.delete(input_notes, 0, axis=0)
    input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)
    prev_start = start

generated_notes = pd.DataFrame(
    generated_notes, columns=(*key_order, 'start', 'end'))

print(generated_notes)

out_file = 'gfgmusicgnerate.mid'
instrument_name= pretty_midi.program_to_instrument_name(instrument.program)
out_pm = notes_to_midi(
	generated_notes, out_file=out_file, instrument_name=instrument_name)
display_audio(out_pm , 500)

